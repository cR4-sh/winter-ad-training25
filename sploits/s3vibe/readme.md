# S3Vibe
## Базовый сценарий
S3-подобное хранилище с веб-интерфейсом. Пользователь может создавать бакеты, загружать файлы, просматривать и скачивать их.

### Все фичи
- Регистрация и авторизация
- Создание бакетов
- Загрузка файлов в бакеты
- Просмотр списка файлов в бакете
- Скачивание файлов

## Архитектура
Микросервисная архитектура:
- `authproxy` - прокси для аутентификации на Python/Twisted
- `s3service` - сервис хранения на Go
- `frontend` - веб-интерфейс на React

## Уязвимости

### HTTP Header Poisoning (разная обработка множественных заголовков)

Критическая уязвимость возникает из-за разного поведения при обработке множественных заголовков с одинаковым именем в разных компонентах системы:

- **Twisted Python (authproxy)** - метод `getHeader()` возвращает **последний** встреченный заголовок
- **Go (s3service)** - метод `r.Header.Get()` возвращает **первый** встреченный заголовок

```python
# authproxy использует Twisted, который берёт ПОСЛЕДНИЙ заголовок
bucket_id = request.getHeader(b's3-bucket-id')  # Последний заголовок
if not bucket_id:
    request.setResponseCode(400)
    # ...
bucket_access = self.auth_manager.check_bucket_access(
    user_info['user_id'],
    bucket_id.decode('utf-8')  # Проверяется доступ к последнему бакету
)
```

При проксировании все заголовки передаются в s3service:
```python
headers = {}
for key, values in request.requestHeaders.getAllRawHeaders():
    headers[key] = values  # Передаются все значения заголовка
```

В s3service используется Go, который берёт ПЕРВЫЙ заголовок:
```go
func (h *Handler) getBucketID(r *http.Request) string {
    return r.Header.Get("s3-bucket-id")  // Первый заголовок
}
```

Таким образом, если отправить два заголовка `s3-bucket-id`:
1. Первый: ID бакета жертвы (будет использован в s3service)
2. Второй: ID своего бакета (будет проверен в authproxy)

authproxy проверит доступ ко второму (своему) бакету и пропустит запрос, а s3service будет работать с первым (бакетом жертвы).

### Local File Read через Path Traversal (обход sanitize.Path)

При листинге объектов используется параметр `prefix`, который передаётся в функцию `ListObjects`. Параметр обрабатывается функцией `sanitize.Path`, но её реализация имеет критические недостатки.

```go
func (h *Handler) ListObjects(w http.ResponseWriter, r *http.Request) {
    bucketID := h.getBucketID(r)
    prefix := r.URL.Query().Get("prefix")  // URL автоматически декодирует %25 в %
    objects, err := h.storage.ListObjects(bucketID, prefix)
    // ...
}
```

В файловой системе prefix конкатенируется с путём бакета и обрабатывается `sanitize.Path`:
```go
func (fs *FileSystemStorage) ListObjects(bucketID, prefix string) ([]ObjectInfo, error) {
    bucketPath := sanitize.Path(fs.basePath + "/" + bucketID)
    searchPath := bucketPath
    if prefix != "" {
        searchPath = sanitize.Path(fs.basePath + "/" + bucketID + "/" + prefix)
    }
    // ...
}
```

#### Анализ функции sanitize.Path

Функция `sanitize.Path` имеет несколько недостатков:

```go
func Path(s string) string {
    filePath := strings.ToLower(s)
    filePath = strings.Replace(filePath, "..", "", -1)  // Удаляет только ".."
    filePath = path.Clean(filePath)  // Нормализует путь
    
    // Разрешает только [:alnum:], ~, -, ., /
    filePath = cleanString(filePath, illegalPath)  // illegalPath = [^[:alnum:]\~\-\./]
    
    return filePath
}
```

**Проблемы:**

1. **Удаление только `..`**: Функция удаляет только точную последовательность `..`, но не защищает от других вариантов обхода, таких как `.%./` (после URL-декодирования `.%25./` становится `.%./`).

2. **Порядок операций**: Функция применяется к уже сконкатенированной строке `basePath + "/" + bucketID + "/" + prefix`. Если в `prefix` передать `.%25./victim_bucket/`, то после URL-декодирования получится `.%./victim_bucket/`, и полный путь станет `/storage/my_bucket/.%./victim_bucket/`.

3. **path.Clean() не защищает**: Функция `path.Clean()` нормализует пути, но при наличии `.%./` в середине пути может не обработать это корректно, особенно если это часть более сложной конструкции.

4. **Регулярное выражение разрешает `.` и `/`**: `illegalPath = [^[:alnum:]\~\-\./]` разрешает использование `.` и `/`, что позволяет создавать пути с `.%./`.

5. **Отсутствие проверки выхода за пределы**: Функция не проверяет, что результирующий путь остаётся в пределах ожидаемой директории бакета.

#### Эксплуатация

Используя `prefix=.%25./{victim_bucket_id}/`, после URL-декодирования получается `.%./{victim_bucket_id}/`. При конкатенации с путём бакета и обработке `sanitize.Path`, можно выйти за пределы текущего бакета и получить доступ к файлам другого бакета.

## Эксплуатация

### HTTP Header Poisoning
1. Создаём свой бакет и получаем auth token
2. Формируем HTTP запрос с двумя заголовками `s3-bucket-id`:
   - **Первый**: ID бакета жертвы из attack_data (будет использован в s3service)
   - **Второй**: ID своего бакета (будет проверен в authproxy)
3. Отправляем запрос через authproxy (порт 8000)
4. authproxy проверяет доступ ко второму заголовку (свой бакет) - доступ разрешён
5. Запрос проксируется в s3service, который использует первый заголовок (бакет жертвы)
6. Получаем доступ к файлам в бакете жертвы

### Local File Read через Path Traversal
1. Получаем ID бакета жертвы из attack_data
2. Используем параметр `prefix` со значением `.%25./{victim_bucket_id}/`:
   - `%25` - URL-encoded символ `%`
   - После декодирования Go получается `.%./{victim_bucket_id}/`
   - При конкатенации: `/storage/my_bucket/.%./victim_bucket/`
3. Функция `sanitize.Path`:
   - Удаляет только `..`, но не `.%./`
   - `path.Clean()` может не обработать `.%./` корректно
   - Регулярное выражение разрешает `.` и `/`
   - Результирующий путь позволяет выйти за пределы бакета
4. Листим файлы в бакете жертвы через обход директорий
5. Скачиваем файлы, используя тот же path traversal в пути к объекту (например, `.%25./{victim_bucket_id}/{filepath}`)

## Как фиксить

### HTTP Header Poisoning
- Проверять, что заголовок `s3-bucket-id` присутствует только один раз
- Использовать единообразную обработку заголовков во всех компонентах (всегда брать первый или всегда последний)
- Валидировать bucket_id на стороне s3service, проверяя доступ через authproxy API
- Использовать единую точку проверки доступа
- Нормализовать заголовки перед обработкой (объединять множественные значения или отклонять запросы с несколькими значениями)

### Local File Read через Path Traversal
- **Улучшить sanitize.Path**: 
  - Удалять не только `..`, но и все варианты обхода (`.%./`, `..%2F`, и т.д.)
  - Проверять путь после нормализации на наличие попыток выхода за пределы
  - Запрещать использование `%` и других специальных символов в путях
- **Валидация после обработки**: После применения `sanitize.Path` проверять, что результирующий путь начинается с пути бакета и не содержит попыток выхода за его пределы
- **Использовать filepath.Join**: Вместо конкатенации строк использовать `filepath.Join`, который безопасно обрабатывает пути
- **Проверка относительного пути**: Использовать `filepath.Rel()` для проверки, что результирующий путь действительно находится внутри директории бакета
- **Белый список символов**: Ограничить разрешённые символы в `prefix` только необходимыми для имён файлов
- **Нормализация перед проверкой**: Применять URL-декодирование и нормализацию пути перед проверкой на path traversal

